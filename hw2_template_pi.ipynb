{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPupEWvXzU+mzXUnQmhpfgC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/karrin10/Machine-Learning-for-the-Internet-of-Things/blob/Homework-2/hw2_template_pi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "> Kathleen Arrington\n",
        "\n",
        "> HW2\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "a5nNqtLKBVV8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import Input, layers, models, Model\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import numpy as np\n",
        "from PIL import Image"
      ],
      "metadata": {
        "id": "hrv0y71b4_P_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define model1\n",
        "def build_model1(input_shape=(32, 32, 3)):\n",
        "    model1 = tf.keras.Sequential([\n",
        "        layers.Input(shape=input_shape),\n",
        "        layers.Conv2D(32, kernel_size=(3,3), strides=(2,2), activation=\"relu\", padding='same'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Conv2D(64, kernel_size=(3,3), strides=(2,2), activation=\"relu\", padding='same'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Conv2D(128, kernel_size=(3,3), strides=(2,2), activation=\"relu\", padding='same'),\n",
        "        layers.BatchNormalization()\n",
        "    ])\n",
        "\n",
        "    # Add four more pairs: Conv2D + BatchNorm\n",
        "    for _ in range(4):\n",
        "        model1.add(layers.Conv2D(128, kernel_size=(3,3), activation=\"relu\", padding='same'))\n",
        "        model1.add(layers.BatchNormalization())\n",
        "\n",
        "    model1.add(layers.MaxPooling2D(pool_size=(4, 4)))\n",
        "    model1.add(layers.Flatten())\n",
        "    model1.add(layers.Dense(128, activation='relu'))\n",
        "    model1.add(layers.BatchNormalization())\n",
        "    model1.add(layers.Dense(10, activation='softmax'))\n",
        "\n",
        "    return model1"
      ],
      "metadata": {
        "id": "p3q7gHrc5Mnb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define model2\n",
        "def build_model2(input_shape=(32, 32, 3)):\n",
        "    model2 = tf.keras.Sequential([\n",
        "        layers.Input(shape=input_shape),\n",
        "        layers.Conv2D(32, kernel_size=(3,3), strides=(2,2), activation=\"relu\", padding='same'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.SeparableConv2D(64, kernel_size=(3,3), strides=(2,2), activation=\"relu\", padding='same'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.SeparableConv2D(128, kernel_size=(3,3), strides=(2,2), activation=\"relu\", padding='same'),\n",
        "        layers.BatchNormalization()\n",
        "    ])\n",
        "\n",
        "    # Add four more pairs: Conv2D + BatchNorm\n",
        "    for _ in range(4):\n",
        "        model2.add(layers.SeparableConv2D(128, kernel_size=(3,3), strides=(2,2), activation=\"relu\", padding='same'))\n",
        "        model2.add(layers.BatchNormalization())\n",
        "\n",
        "    model2.add(layers.MaxPooling2D(pool_size=(1, 1)))\n",
        "    model2.add(layers.Flatten())\n",
        "    model2.add(layers.Dense(128, activation='relu'))\n",
        "    model2.add(layers.BatchNormalization())\n",
        "    model2.add(layers.Dense(10, activation='softmax'))\n",
        "\n",
        "    return model2"
      ],
      "metadata": {
        "id": "gyiq4u3O5YgE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define model3\n",
        "def build_model3(input_shape=(32, 32, 3)):\n",
        "    # Apply input layer\n",
        "    inputs = tf.keras.Input(shape=input_shape)\n",
        "\n",
        "    # Incorporate convolutional block\n",
        "    x = layers.Conv2D(32, kernel_size=(3,3), strides=(2,2), activation=\"relu\", padding='same')(inputs)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "\n",
        "    shortcut = x\n",
        "\n",
        "    x = layers.Conv2D(64, kernel_size=(3,3), strides=(2,2), activation=\"relu\", padding='same')(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Dropout(0.2)(x)\n",
        "\n",
        "    x = layers.Conv2D(128, kernel_size=(3,3), strides=(2,2), activation=\"relu\", padding='same')(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "\n",
        "    # After the fourth convolutional layer, skip connection\n",
        "    shortcut = layers.Conv2D(128, kernel_size=(1, 1), strides=(4, 4), padding='same')(shortcut)\n",
        "    x = layers.Add()([x, shortcut])\n",
        "    shortcut2 = x\n",
        "    # Add four more pairs: Conv2D + BatchNorm + Dropout\n",
        "    for _ in range(2):\n",
        "        x = layers.Conv2D(128, kernel_size=(3,3), activation=\"relu\", padding='same')(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "        x = layers.Dropout(0.2)(x)\n",
        "    x = layers.Add()([x, shortcut2])\n",
        "    shortcut3 = x\n",
        "    for _ in range(2):\n",
        "        x = layers.Conv2D(128, kernel_size=(3,3), activation=\"relu\", padding='same')(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "        x = layers.Dropout(0.2)(x)\n",
        "    x = layers.Add()([x, shortcut3])\n",
        "\n",
        "\n",
        "    x = layers.MaxPooling2D(pool_size=(4, 4))(x)\n",
        "    x = layers.Flatten()(x)\n",
        "    x = layers.Dense(128, activation='relu')(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    outputs = layers.Dense(10, activation='softmax')(x)\n",
        "\n",
        "    # Set model3\n",
        "    model3 = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "    return model3"
      ],
      "metadata": {
        "id": "yKooU1LG5cv8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define model50k\n",
        "def build_model50k(input_shape=(32, 32, 3), dropout_rate=0.2):\n",
        "    model50k = tf.keras.Sequential([\n",
        "        layers.Input(shape=input_shape),\n",
        "        layers.DepthwiseConv2D(kernel_size=(3,3), padding='same'),\n",
        "        layers.Conv2D(32, kernel_size=(1,1), activation=\"relu\", padding='same'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dropout(dropout_rate),\n",
        "        layers.DepthwiseConv2D(kernel_size=(3,3), padding='same'),\n",
        "        layers.Conv2D(64, kernel_size=(1,1), strides=(4,4), activation=\"relu\", padding='same'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dropout(dropout_rate),\n",
        "        layers.DepthwiseConv2D(kernel_size=(3,3), padding='same'),\n",
        "        layers.Conv2D(96, kernel_size=(1,1), activation=\"relu\", padding='same'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dropout(dropout_rate)\n",
        "    ])\n",
        "    model50k.add(layers.MaxPooling2D(pool_size=(4, 4)))\n",
        "    model50k.add(layers.Flatten())\n",
        "    model50k.add(layers.Dense(96, activation='relu'))\n",
        "    model50k.add(layers.BatchNormalization())\n",
        "    model50k.add(layers.Dropout(dropout_rate))\n",
        "    model50k.add(layers.Dense(10, activation='softmax'))\n",
        "    return model50k"
      ],
      "metadata": {
        "id": "tIBCCiM75lPJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NGjagjEx5toI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-vMMRYGrd-1L",
        "outputId": "53cc3935-b0a0-4b5b-f0bc-749f663de44b"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170498071/170498071 [==============================] - 2s 0us/step\n",
            "Epoch 1/50\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/backend.py:5727: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1563/1563 [==============================] - 172s 108ms/step - loss: 1.4752 - accuracy: 0.4656 - val_loss: 1.4797 - val_accuracy: 0.4779\n",
            "Epoch 2/50\n",
            "1563/1563 [==============================] - 167s 107ms/step - loss: 1.0936 - accuracy: 0.6130 - val_loss: 1.1574 - val_accuracy: 0.5959\n",
            "Epoch 3/50\n",
            "1563/1563 [==============================] - 171s 109ms/step - loss: 0.8973 - accuracy: 0.6854 - val_loss: 1.0855 - val_accuracy: 0.6284\n",
            "Epoch 4/50\n",
            "1563/1563 [==============================] - 167s 107ms/step - loss: 0.7395 - accuracy: 0.7413 - val_loss: 0.9950 - val_accuracy: 0.6639\n",
            "Epoch 5/50\n",
            "1563/1563 [==============================] - 167s 107ms/step - loss: 0.6149 - accuracy: 0.7856 - val_loss: 0.8347 - val_accuracy: 0.7192\n",
            "Epoch 6/50\n",
            "1563/1563 [==============================] - 167s 107ms/step - loss: 0.5079 - accuracy: 0.8223 - val_loss: 0.9777 - val_accuracy: 0.6858\n",
            "Epoch 7/50\n",
            "1563/1563 [==============================] - 169s 108ms/step - loss: 0.4094 - accuracy: 0.8570 - val_loss: 0.8826 - val_accuracy: 0.7230\n",
            "Epoch 8/50\n",
            "1563/1563 [==============================] - 160s 102ms/step - loss: 0.3268 - accuracy: 0.8861 - val_loss: 0.9284 - val_accuracy: 0.7306\n",
            "Epoch 9/50\n",
            "1563/1563 [==============================] - 163s 104ms/step - loss: 0.2731 - accuracy: 0.9041 - val_loss: 0.9635 - val_accuracy: 0.7189\n",
            "Epoch 10/50\n",
            "1563/1563 [==============================] - 171s 109ms/step - loss: 0.2240 - accuracy: 0.9228 - val_loss: 1.0082 - val_accuracy: 0.7282\n",
            "Epoch 11/50\n",
            "1563/1563 [==============================] - 164s 105ms/step - loss: 0.1972 - accuracy: 0.9311 - val_loss: 1.1954 - val_accuracy: 0.7004\n",
            "Epoch 12/50\n",
            "1563/1563 [==============================] - 165s 105ms/step - loss: 0.1621 - accuracy: 0.9436 - val_loss: 1.1159 - val_accuracy: 0.7359\n",
            "Epoch 13/50\n",
            "1563/1563 [==============================] - 164s 105ms/step - loss: 0.1555 - accuracy: 0.9469 - val_loss: 1.2256 - val_accuracy: 0.7200\n",
            "Epoch 14/50\n",
            "1563/1563 [==============================] - 163s 104ms/step - loss: 0.1403 - accuracy: 0.9514 - val_loss: 1.1875 - val_accuracy: 0.7227\n",
            "Epoch 15/50\n",
            "1563/1563 [==============================] - 166s 106ms/step - loss: 0.1296 - accuracy: 0.9548 - val_loss: 1.1936 - val_accuracy: 0.7279\n",
            "Epoch 16/50\n",
            "1563/1563 [==============================] - 163s 104ms/step - loss: 0.1154 - accuracy: 0.9597 - val_loss: 1.3066 - val_accuracy: 0.7214\n",
            "Epoch 17/50\n",
            "1563/1563 [==============================] - 163s 105ms/step - loss: 0.1084 - accuracy: 0.9624 - val_loss: 1.3886 - val_accuracy: 0.7201\n",
            "Epoch 18/50\n",
            "1563/1563 [==============================] - 166s 106ms/step - loss: 0.1059 - accuracy: 0.9630 - val_loss: 1.3241 - val_accuracy: 0.7257\n",
            "Epoch 19/50\n",
            "1563/1563 [==============================] - 167s 107ms/step - loss: 0.0961 - accuracy: 0.9668 - val_loss: 1.3722 - val_accuracy: 0.7185\n",
            "Epoch 20/50\n",
            "1563/1563 [==============================] - 166s 106ms/step - loss: 0.0920 - accuracy: 0.9683 - val_loss: 1.3646 - val_accuracy: 0.7269\n",
            "Epoch 21/50\n",
            "1563/1563 [==============================] - 163s 104ms/step - loss: 0.0886 - accuracy: 0.9700 - val_loss: 1.3671 - val_accuracy: 0.7179\n",
            "Epoch 22/50\n",
            "1563/1563 [==============================] - 167s 107ms/step - loss: 0.0800 - accuracy: 0.9728 - val_loss: 1.3399 - val_accuracy: 0.7324\n",
            "Epoch 23/50\n",
            "1563/1563 [==============================] - 162s 104ms/step - loss: 0.0780 - accuracy: 0.9737 - val_loss: 1.4183 - val_accuracy: 0.7244\n",
            "Epoch 24/50\n",
            "1563/1563 [==============================] - 163s 104ms/step - loss: 0.0749 - accuracy: 0.9744 - val_loss: 1.4125 - val_accuracy: 0.7291\n",
            "Epoch 25/50\n",
            "1563/1563 [==============================] - 166s 106ms/step - loss: 0.0721 - accuracy: 0.9749 - val_loss: 1.4309 - val_accuracy: 0.7205\n",
            "Epoch 26/50\n",
            "1563/1563 [==============================] - 160s 103ms/step - loss: 0.0723 - accuracy: 0.9758 - val_loss: 1.4422 - val_accuracy: 0.7205\n",
            "Epoch 27/50\n",
            "1563/1563 [==============================] - 163s 104ms/step - loss: 0.0688 - accuracy: 0.9761 - val_loss: 1.4700 - val_accuracy: 0.7248\n",
            "Epoch 28/50\n",
            "1563/1563 [==============================] - 168s 108ms/step - loss: 0.0642 - accuracy: 0.9777 - val_loss: 1.3655 - val_accuracy: 0.7369\n",
            "Epoch 29/50\n",
            "1563/1563 [==============================] - 164s 105ms/step - loss: 0.0583 - accuracy: 0.9799 - val_loss: 1.4578 - val_accuracy: 0.7255\n",
            "Epoch 30/50\n",
            "1563/1563 [==============================] - 161s 103ms/step - loss: 0.0617 - accuracy: 0.9780 - val_loss: 1.5131 - val_accuracy: 0.7293\n",
            "Epoch 31/50\n",
            "1563/1563 [==============================] - 159s 102ms/step - loss: 0.0615 - accuracy: 0.9782 - val_loss: 1.4724 - val_accuracy: 0.7298\n",
            "Epoch 32/50\n",
            "1563/1563 [==============================] - 157s 101ms/step - loss: 0.0544 - accuracy: 0.9811 - val_loss: 1.5265 - val_accuracy: 0.7216\n",
            "Epoch 33/50\n",
            "1563/1563 [==============================] - 157s 101ms/step - loss: 0.0535 - accuracy: 0.9821 - val_loss: 1.4953 - val_accuracy: 0.7321\n",
            "Epoch 34/50\n",
            "1563/1563 [==============================] - 158s 101ms/step - loss: 0.0515 - accuracy: 0.9825 - val_loss: 1.5191 - val_accuracy: 0.7320\n",
            "Epoch 35/50\n",
            "1563/1563 [==============================] - 160s 103ms/step - loss: 0.0518 - accuracy: 0.9820 - val_loss: 1.6539 - val_accuracy: 0.7170\n",
            "Epoch 36/50\n",
            "1563/1563 [==============================] - 155s 99ms/step - loss: 0.0491 - accuracy: 0.9834 - val_loss: 1.4904 - val_accuracy: 0.7389\n",
            "Epoch 37/50\n",
            "1563/1563 [==============================] - 160s 102ms/step - loss: 0.0465 - accuracy: 0.9839 - val_loss: 1.4885 - val_accuracy: 0.7320\n",
            "Epoch 38/50\n",
            "1563/1563 [==============================] - 161s 103ms/step - loss: 0.0482 - accuracy: 0.9834 - val_loss: 1.5077 - val_accuracy: 0.7356\n",
            "Epoch 39/50\n",
            "1563/1563 [==============================] - 163s 104ms/step - loss: 0.0437 - accuracy: 0.9850 - val_loss: 1.6038 - val_accuracy: 0.7295\n",
            "Epoch 40/50\n",
            "1563/1563 [==============================] - 160s 102ms/step - loss: 0.0471 - accuracy: 0.9847 - val_loss: 1.5056 - val_accuracy: 0.7332\n",
            "Epoch 41/50\n",
            "1563/1563 [==============================] - 164s 105ms/step - loss: 0.0411 - accuracy: 0.9860 - val_loss: 1.5973 - val_accuracy: 0.7388\n",
            "Epoch 42/50\n",
            "1563/1563 [==============================] - 163s 104ms/step - loss: 0.0431 - accuracy: 0.9854 - val_loss: 1.6215 - val_accuracy: 0.7253\n",
            "Epoch 43/50\n",
            "1563/1563 [==============================] - 165s 105ms/step - loss: 0.0433 - accuracy: 0.9853 - val_loss: 1.6278 - val_accuracy: 0.7280\n",
            "Epoch 44/50\n",
            "1563/1563 [==============================] - 159s 102ms/step - loss: 0.0377 - accuracy: 0.9875 - val_loss: 1.6007 - val_accuracy: 0.7369\n",
            "Epoch 45/50\n",
            "1563/1563 [==============================] - 159s 102ms/step - loss: 0.0403 - accuracy: 0.9863 - val_loss: 1.5750 - val_accuracy: 0.7302\n",
            "Epoch 46/50\n",
            "1563/1563 [==============================] - 163s 104ms/step - loss: 0.0383 - accuracy: 0.9869 - val_loss: 1.6323 - val_accuracy: 0.7245\n",
            "Epoch 47/50\n",
            "1563/1563 [==============================] - 159s 102ms/step - loss: 0.0402 - accuracy: 0.9865 - val_loss: 1.5840 - val_accuracy: 0.7312\n",
            "Epoch 48/50\n",
            "1563/1563 [==============================] - 156s 100ms/step - loss: 0.0374 - accuracy: 0.9875 - val_loss: 1.6350 - val_accuracy: 0.7333\n",
            "Epoch 49/50\n",
            "1563/1563 [==============================] - 161s 103ms/step - loss: 0.0359 - accuracy: 0.9877 - val_loss: 1.6685 - val_accuracy: 0.7307\n",
            "Epoch 50/50\n",
            "1563/1563 [==============================] - 164s 105ms/step - loss: 0.0370 - accuracy: 0.9873 - val_loss: 1.6464 - val_accuracy: 0.7365\n",
            "1563/1563 - 34s - loss: 0.0159 - accuracy: 0.9946 - 34s/epoch - 22ms/step\n"
          ]
        },
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'test_image_model1.png'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-fd70fa18caf8>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m#image processing test for model 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m   \u001b[0mimage_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"test_image_model1.png\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m   \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m   \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3226\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3227\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3228\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'test_image_model1.png'"
          ]
        }
      ],
      "source": [
        "# no training or dataset construction should happen above this line\n",
        "if __name__ == '__main__':\n",
        "\n",
        "  ########################################\n",
        "  (train_images, train_labels), (test_images, test_labels) = cifar10.load_data()\n",
        "  class_names = ['airplane','automobile','bird','cat','deer','dog','frog','horse','ship','truck']\n",
        "  train_labels = train_labels.squeeze()\n",
        "  test_labels = test_labels.squeeze()\n",
        "\n",
        "  input_shape = train_images.shape[1:]\n",
        "  train_images = train_images / 255.0\n",
        "  test_images = test_images / 255.0\n",
        "\n",
        "  ########################################\n",
        "  ## Build and train model 1\n",
        "  model1 = build_model1()\n",
        "  # compile and train model 1.\n",
        "  # Train the model\n",
        "  model1.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "  history = model1.fit(train_images, train_labels, epochs=50, validation_data=(test_images, test_labels))\n",
        "  test_loss, test_acc = model1.evaluate(train_images, train_labels, verbose=2)\n",
        "\n",
        "#image processing test for model 1\n",
        "  image_path = \"test_image_model1.png\"\n",
        "  image = Image.open(image_path)\n",
        "  image = image.resize((32, 32))\n",
        "\n",
        "  if image.mode != 'RGB':\n",
        "    image = image.convert('RGB')\n",
        "\n",
        "  # Convert image to numpy array and normalize\n",
        "  image_array = np.array(image) / 255.0  # Normalize pixel values to [0, 1]\n",
        "\n",
        "  # Expand dimensions to match the input shape expected by the model\n",
        "  image_array = np.expand_dims(image_array, axis=0)\n",
        "\n",
        "  # Make predictions\n",
        "  predictions = model1.predict(image_array)\n",
        "\n",
        "  # Get the predicted class\n",
        "  predicted_class = np.argmax(predictions[0])\n",
        "\n",
        "  # Print the predicted class\n",
        "  print(\"Predicted class:\", predicted_class)\n",
        "\n",
        "#########################################\n",
        "  ## Build, compile, and train model 2 (DS Convolutions)\n",
        "  model2 = build_model2()\n",
        "  # compile and train model 1.\n",
        "  # Train the model\n",
        "  model2.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "  history2 = model2.fit(train_images, train_labels, epochs=50, validation_data=(test_images, test_labels))\n",
        "  test_loss, test_acc = model2.evaluate(train_images, train_labels, verbose=2)\n",
        "\n",
        "#image processing test for model 2\n",
        "  image_path = \"test_image_model2.png\"\n",
        "  image = Image.open(image_path)\n",
        "  image = image.resize((32, 32))\n",
        "\n",
        "  if image.mode != 'RGB':\n",
        "    image = image.convert('RGB')\n",
        "\n",
        "  # Convert image to numpy array and normalize\n",
        "  image_array = np.array(image) / 255.0  # Normalize pixel values to [0, 1]\n",
        "\n",
        "  # Expand dimensions to match the input shape expected by the model\n",
        "  image_array = np.expand_dims(image_array, axis=0)\n",
        "\n",
        "  # Make predictions\n",
        "  predictions = model2.predict(image_array)\n",
        "\n",
        "  # Get the predicted class\n",
        "  predicted_class = np.argmax(predictions[0])\n",
        "\n",
        "  # Print the predicted class\n",
        "  print(\"Predicted class:\", predicted_class)\n",
        "\n",
        "#########################################\n",
        "  ### Repeat for model 3\n",
        "  ## Build and train model 1\n",
        "  model3 = build_model3()\n",
        "  # compile and train model 1.\n",
        "  # Train the model\n",
        "  model3.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "  history = model3.fit(train_images, train_labels, epochs=50, validation_data=(test_images, test_labels))\n",
        "  test_loss, test_acc = model3.evaluate(train_images, train_labels, verbose=2)\n",
        "\n",
        "#image processing test for model 3\n",
        "  image_path = \"test_image_model3.png\"\n",
        "  image = Image.open(image_path)\n",
        "  image = image.resize((32, 32))\n",
        "\n",
        "  if image.mode != 'RGB':\n",
        "    image = image.convert('RGB')\n",
        "\n",
        "  # Convert image to numpy array and normalize\n",
        "  image_array = np.array(image) / 255.0  # Normalize pixel values to [0, 1]\n",
        "\n",
        "  # Expand dimensions to match the input shape expected by the model\n",
        "  image_array = np.expand_dims(image_array, axis=0)\n",
        "\n",
        "  # Make predictions\n",
        "  predictions = model1.predict(image_array)\n",
        "\n",
        "  # Get the predicted class\n",
        "  predicted_class = np.argmax(predictions[0])\n",
        "\n",
        "  # Print the predicted class\n",
        "  print(\"Predicted class:\", predicted_class)\n",
        "\n",
        "#########################################\n",
        "  ### Repeat for model 50k\n",
        "  ## Build and train model 50k\n",
        "  model50k = build_model50k()\n",
        "  # compile and train model 50k\n",
        "  # Train the model\n",
        "  model50k.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "  history = model50k.fit(train_images, train_labels, epochs=50, validation_data=(test_images, test_labels))\n",
        "  test_loss, test_acc = model50k.evaluate(train_images, train_labels, verbose=2)\n",
        "  # hw2_complete.py snippet for training and saving model50k\n",
        "  model50k = build_model50k(input_shape=(32, 32, 3), dropout_rate=0.2)\n",
        "  # Assume model50k is compiled and fitted here\n",
        "  model50k.save(\"best_model.h5\")\n",
        "\n",
        "\n",
        "#image processing test for model 50k\n",
        "  image_path = \"Airplane.png\"\n",
        "  image = Image.open(image_path)\n",
        "  image = image.resize((32, 32))\n",
        "\n",
        "  if image.mode != 'RGB':\n",
        "    image = image.convert('RGB')\n",
        "\n",
        "  # Convert image to numpy array and normalize\n",
        "  image_array = np.array(image) / 255.0  # Normalize pixel values to [0, 1]\n",
        "\n",
        "  # Expand dimensions to match the input shape expected by the model\n",
        "  image_array = np.expand_dims(image_array, axis=0)\n",
        "\n",
        "  # Make predictions\n",
        "  predictions = model1.predict(image_array)\n",
        "\n",
        "  # Get the predicted class\n",
        "  predicted_class = np.argmax(predictions[0])\n",
        "\n",
        "  # Print the predicted class\n",
        "  print(\"Predicted class:\", predicted_class)"
      ]
    }
  ]
}